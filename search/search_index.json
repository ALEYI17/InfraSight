{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"InfraSight","text":"<p>InfraSight is an observability and auditing platform that uses eBPF to capture low-level system events (like <code>execve</code>, <code>open</code>, <code>connect</code>, etc.) and stream them to a ClickHouse backend for high-performance analysis.</p> <p>It helps platform engineers, SREs, and security teams gain deep visibility into workloads, processes, and container behavior \u2014 on Linux and Kubernetes alike.</p> <p></p>"},{"location":"#key-features","title":"\ud83d\ude80 Key Features","text":"<ul> <li>Fine-grained tracing of Linux syscalls using eBPF</li> <li>Real-time gRPC-based event delivery</li> <li>ClickHouse storage for fast analytics</li> <li>Kubernetes-native agent deployment</li> <li>CRD-based configuration (via <code>EbpfDaemonSet</code>)</li> <li>Extensible probe system</li> <li>Machine learning\u2013based anomaly detection for syscall frequency and resource usage</li> <li>Rules engine (InfraSight Sentinel) for real-time threat detection and alerting</li> </ul> <p>\ud83d\udc49 Get started here or jump to:</p> <ul> <li>eBPF Programs</li> <li>ClickHouse Schema</li> <li>Architecture Overview</li> <li>Kubernetes CRD</li> <li>Live Demo</li> </ul>"},{"location":"architecture/","title":"Architecture","text":""},{"location":"architecture/#architecture-diagram","title":"\ud83d\uddfa\ufe0f Architecture Diagram","text":"<p>The diagram below illustrates the high-level architecture of the InfraSight platform:</p> <p></p> <p>InfraSight is designed to provide detailed monitoring and auditing of Linux systems (both standalone and within Kubernetes clusters) by collecting low-level system events using eBPF. It operates through a modular pipeline that integrates multiple components for high-performance data collection, enrichment, and storage:</p>"},{"location":"architecture/#1-ebpf-agents","title":"1. eBPF Agents","text":"<p>At the core of InfraSight are eBPF Agents, which are lightweight, kernel-level agents running on each monitored node. These agents use eBPF programs attached to various tracepoints and kprobes to capture critical system call events such as:</p> <ul> <li><code>execve</code>: Monitoring process execution (command launches).</li> <li><code>open</code>: Tracking file access and modification.</li> <li><code>chmod</code>: Observing file permission changes.</li> <li><code>connect</code> and <code>accept</code>: Monitoring network connection and acceptance events.</li> <li><code>ptrace</code> \u2192 Detecting process tracing operations (debuggers, tampering, reverse engineering).</li> <li><code>mmap</code> \u2192 Tracking memory mapping activity, including suspicious RWX mappings.</li> <li><code>mount</code> and <code>umount</code> \u2192 Monitoring filesystem mount/unmount activity, often linked to container isolation or tampering.</li> <li><code>Resource</code> \u2192 Measuring process resource usage (CPU time, memory allocations, I/O, page faults, and process lifecycle events).</li> <li><code>SyscallFreq</code> \u2192 Aggregating syscall invocation counts per process, useful for anomaly detection based on unusual syscall usage patterns.</li> </ul> <p>These eBPF programs operate directly within the Linux kernel, providing efficient and real-time event tracing with minimal overhead. The eBPF Agents collect raw data, including process information, network events, and file system interactions.</p>"},{"location":"architecture/#2-infrasight-server","title":"2. InfraSight Server","text":"<p>The InfraSight Server serves as the central processing unit, receiving the raw events from the eBPF Agents. Upon receiving the data, the server performs additional enrichment, such as:</p> <ul> <li>Converting latency from nanoseconds to more understandable units such as milliseconds or seconds.</li> <li>Further converting event data (e.g., converting network protocol identifiers, if necessary).</li> <li>Formatting timestamps to ISO8601 for easier readability.</li> </ul> <p>Once enriched, the data is stored in a high-performance, scalable database.</p>"},{"location":"architecture/#3-kafka-message-bus","title":"3. Kafka Message Bus","text":"<p>Apache Kafka acts as the event distribution backbone of InfraSight. By publishing enriched eBPF events into Kafka, the platform allows multiple consumers to subscribe and process the data independently. This decouples data collection from detection and analysis, enabling scalability and modularity.</p>"},{"location":"architecture/#4-detection-analytics-components","title":"4. Detection &amp; Analytics Components","text":"<p>InfraSight provides multiple detection layers consuming data from Kafka:</p>"},{"location":"architecture/#infrasight-sentinel-rules-engine","title":"\ud83d\udd39 InfraSight Sentinel (Rules Engine)","text":"<p>The InfraSight Sentinel is the rules engine that evaluates eBPF events in real time. Its goal is to detect suspicious behaviors such as fileless execution, privilege escalation, or unusual system activity.  </p> <p>It is responsible for: - Consuming events from Kafka produced by the InfraSight server. - Applying security rules (based on syscalls and enriched process/container context). - Generating structured alerts with detailed process, container, and user information. - Integrating seamlessly with the rest of the InfraSight ecosystem.  </p>"},{"location":"architecture/#anomaly-detection-ml-models","title":"\ud83d\udd39 Anomaly Detection (ML Models)","text":"<p>InfraSight also includes machine learning\u2013based anomaly detection modules:</p> <ul> <li><code>infrasight-resource</code> \u2192 Focuses on detecting anomalies in resource usage patterns (CPU, memory, faults, etc).  </li> <li><code>infrasight-syscall</code> \u2192 Detects anomalies in syscall frequency and behavior, identifying deviations from learned baselines.  </li> </ul> <p>These models consume Kafka events, apply per-container and global anomaly detection, and produce alerts or insights. They complement Sentinel by identifying subtle or previously unknown threats.</p>"},{"location":"architecture/#5-clickhouse-database","title":"5. ClickHouse Database","text":"<p>InfraSight uses ClickHouse, a columnar database optimized for fast analytical queries, as the backend to store and manage the collected events. ClickHouse allows efficient querying even with large volumes of data, which is essential for event-driven telemetry systems.</p> <p>Data stored in ClickHouse includes system call events, enriched metadata, and timestamped information that can be queried and analyzed in real-time.</p>"},{"location":"architecture/#5-data-analysis-and-visualization","title":"5. Data Analysis and Visualization","text":"<p>The final step of the pipeline involves visualizing and analyzing the collected events. InfraSight supports integration with Grafana (or similar tools) to build customizable dashboards. Users can query and analyze the data stored in ClickHouse to extract valuable insights, such as:</p> <ul> <li>Security audit logs: Understanding access to sensitive files or network connections.</li> <li>Performance monitoring: Tracking resource usage by processes, network, and file system.</li> <li>Anomaly detection: Identifying unusual patterns of behavior based on syscall events.</li> </ul>"},{"location":"architecture/#6-flexibility-in-deployment","title":"6. Flexibility in Deployment","text":"<p>This architecture is designed to work both in standalone Linux environments and in Kubernetes clusters. The modularity allows easy deployment:</p> <ul> <li>On Kubernetes: The controller (<code>infrasight-controller</code>) can be used to deploy eBPF agents (as DaemonSets), manage their configuration, and collect telemetry from all nodes in the cluster.</li> <li>On standalone Linux hosts: The eBPF agents can run on any Linux-based system, providing valuable insights in non-Kubernetes environments.</li> </ul>"},{"location":"crd/","title":"Kubernetes CRD","text":""},{"location":"crd/#custom-resource-definition-crd","title":"\ud83d\udce6 Custom Resource Definition (CRD)","text":"<p>InfraSight provides a Kubernetes-native way to manage and deploy its eBPF agent using a Custom Resource Definition (CRD) called <code>EbpfDaemonSet</code>. This resource allows you to declaratively configure which probes to run, where to send data, and how to control runtime behavior.</p> <p>The CRD and its controller logic are defined in the infrasight-controller repository. This controller watches for changes to <code>EbpfDaemonSet</code> resources and reconciles them into actual <code>DaemonSet</code> objects that run the configured probes on cluster nodes.</p>"},{"location":"crd/#example-cr","title":"\ud83d\udcc4 Example CR","text":"<p>Here\u2019s an example <code>EbpfDaemonSet</code> resource that enables the <code>connect</code> and <code>accept</code> probes and sends data to the InfraSight server:</p> <p>\ud83d\udd17 View on GitHub</p> <pre><code>apiVersion: ebpf.monitoring.dev/v1alpha1\nkind: EbpfDaemonSet\nmetadata:\n  labels:\n    app.kubernetes.io/name: kube-ebpf-monitor\n    app.kubernetes.io/managed-by: kustomize\n  name: ebpfdaemonset-sample\nspec:\n  image: ghcr.io/aleyi17/ebpf_loader:latest\n  nodeSelector:\n    kubernetes.io/os: linux \n  enableProbes:\n    - connect\n    - accept\n  serverPort: \"8080\"\n  serverAddress: ebpfplatform-ebpf-server\n  prometheusPort: \"9090\" \n</code></pre>"},{"location":"crd/#field-reference","title":"\ud83e\uddfe Field Reference","text":"Field Type Description <code>image</code> <code>string</code> Container image to run (e.g., the eBPF loader agent). <code>nodeSelector</code> <code>map[string]string</code> Restrict the DaemonSet to specific nodes. Typically used to target Linux nodes. <code>tolerations</code> <code>[]Toleration</code> Tolerations for scheduling on tainted nodes (optional). <code>resources</code> <code>ResourceRequirements</code> CPU and memory requests/limits for the eBPF agent pods (optional). <code>runPrivileged</code> <code>bool</code> If set to true, runs the container in privileged mode. Required for most eBPF operations. <code>enableProbes</code> <code>[]string</code> List of enabled eBPF probes (e.g., <code>execve</code>, <code>connect</code>, <code>accept</code>, etc.). <code>serverAddress</code> <code>string</code> The address of the InfraSight server to send data to (typically a Kubernetes service name or IP). <code>serverPort</code> <code>string</code> The gRPC port on which the InfraSight server is listening. <code>prometheusPort</code> <code>string</code> The HTTP port on which the agent exposes Prometheus metrics (e.g., <code>9090</code>)."},{"location":"database-schema/","title":"\ud83e\uddec Database Schema","text":"<p>InfraSight stores all enriched telemetry data in ClickHouse, using two primary tables: <code>tracing_events</code> and <code>network_events</code>. Below you\u2019ll find their schemas and a breakdown of the meaning of each field.</p>"},{"location":"database-schema/#tracing_events-table","title":"\ud83d\udcc1 <code>tracing_events</code> Table","text":"Click to show table schema <pre><code>CREATE TABLE IF NOT EXISTS audit.tracing_events (\n  pid UInt32,\n  uid UInt32,\n  gid UInt32,\n  ppid UInt32,\n  user_pid UInt32,\n  user_ppid UInt32,\n  cgroup_id UInt64,\n  cgroup_name String,\n  comm String,\n  filename String,\n  monotonic_ts_enter_ns UInt64,\n  monotonic_ts_exit_ns UInt64,\n  return_code Int64,\n  latency_ns UInt64,\n  event_type String,\n  node_name String,\n  user String,\n  latency_ms Float64, \n  wall_time_ms Int64,\n  wall_time_dt DateTime64(3),\n  container_id String,\n  container_image String,\n  container_labels_json JSON\n)\nENGINE = MergeTree()\nORDER BY wall_time_ms;\n</code></pre>"},{"location":"database-schema/#field-descriptions","title":"\ud83d\udd0d Field Descriptions","text":"Field Description <code>pid</code> Process ID of the event-emitting process <code>uid</code> User ID under which the process is running <code>gid</code> Group ID of the process <code>ppid</code> Parent Process ID <code>user_pid</code> Userspace PID as seen by the process itself (may differ in containers) <code>user_ppid</code> Userspace PPID from the process's PID namespace <code>cgroup_id</code> CGroup ID the process belongs to <code>cgroup_name</code> Human-readable name or resolved path of the cgroup <code>comm</code> Command name (basename of the process) <code>filename</code> Name of the file involved in the syscall (e.g., for <code>open</code>) <code>monotonic_ts_enter_ns</code> Timestamp (monotonic clock) when the syscall started (in nanoseconds) <code>monotonic_ts_exit_ns</code> Timestamp when the syscall exited (in nanoseconds) <code>return_code</code> Return value of the syscall <code>latency_ns</code> Duration of the syscall in nanoseconds <code>event_type</code> Type of syscall event (e.g., <code>execve</code>, <code>open</code>, <code>chmod</code>, etc.) <code>node_name</code> Hostname of the node where the event occurred <code>user</code> Username resolved from the UID <code>latency_ms</code> Latency converted to milliseconds <code>wall_time_ms</code> Wall-clock timestamp (milliseconds since epoch) <code>wall_time_dt</code> ISO8601-formatted timestamp with millisecond precision <code>container_id</code> Container ID, if the process is running in a container <code>container_image</code> Name of the container image, if available <code>container_labels_json</code> Labels from the container, stored as JSON"},{"location":"database-schema/#network_events-table","title":"\ud83c\udf10 <code>network_events</code> Table","text":"Click to show table schema <pre><code>CREATE TABLE IF NOT EXISTS audit.network_events (\n  pid UInt32,\n  uid UInt32,\n  gid UInt32,\n  ppid UInt32,\n  user_pid UInt32,\n  user_ppid UInt32,\n  cgroup_id UInt64,\n  cgroup_name String,\n  comm String,\n\n  sa_family String,\n  saddr_ipv4 String,\n  daddr_ipv4 String,\n  sport String,\n  dport String,\n  saddr_ipv6 String,\n  daddr_ipv6 String,\n  resolved_domain Nullable(String),\n  monotonic_ts_enter_ns UInt64,\n  monotonic_ts_exit_ns UInt64,\n  return_code Int64,\n  latency_ns UInt64,\n\n  event_type String,\n  node_name String,\n  user String,\n\n  latency_ms Float64,\n  wall_time_ms Int64,\n  wall_time_dt DateTime64(3),\n\n  container_id String,\n  container_image String,\n  container_labels_json JSON\n)\nENGINE = MergeTree()\nORDER BY wall_time_ms;\n</code></pre>"},{"location":"database-schema/#field-descriptions_1","title":"\ud83d\udd0d Field Descriptions","text":"<p>Includes all common fields described above, plus:</p> Field Description <code>sa_family</code> Socket address family (e.g., <code>AF_INET</code>, <code>AF_INET6</code>, <code>AF_UNIX</code>) <code>saddr_ipv4</code> Source IPv4 address (if applicable) <code>daddr_ipv4</code> Destination IPv4 address (if applicable) <code>sport</code> Source port <code>dport</code> Destination port <code>saddr_ipv6</code> Source IPv6 address (if applicable) <code>daddr_ipv6</code> Destination IPv6 address (if applicable) <code>resolved_domain</code> Reverse-resolved domain of the destination IP (if public and resolvable); <code>NULL</code> if not resolvable or internal/private IP"},{"location":"database-schema/#ptrace_events-table","title":"\ud83e\udde9 <code>ptrace_events</code> Table","text":"Click to show table schema <pre><code>CREATE TABLE IF NOT EXISTS audit.ptrace_events (\n  pid UInt32,\n  uid UInt32,\n  gid UInt32,\n  ppid UInt32,\n  user_pid UInt32,\n  user_ppid UInt32,\n  cgroup_id UInt64,\n  cgroup_name String,\n  comm String,\n\n  request Int64,\n  target_pid Int64,\n  addr UInt64,\n  data UInt64,\n  request_name String,\n  monotonic_ts_enter_ns UInt64,\n  monotonic_ts_exit_ns UInt64,\n  return_code Int64,\n  latency_ns UInt64,\n\n  event_type String,\n  node_name String,\n  user String,\n\n  latency_ms Float64,\n  wall_time_ms Int64,\n  wall_time_dt DateTime64(3),\n\n  container_id String,\n  container_image String,\n  container_labels_json JSON\n)\nENGINE = MergeTree()\nORDER BY wall_time_ms;\n</code></pre>"},{"location":"database-schema/#field-descriptions_2","title":"\ud83d\udd0d Field Descriptions","text":"<p>Includes all common fields described above, plus:</p> Field Description <code>request</code> Raw numeric value passed to <code>ptrace</code> (e.g., <code>0</code>, <code>16</code>, <code>24</code>) <code>target_pid</code> The PID of the target process being traced <code>addr</code> Memory address or register offset used in the <code>ptrace</code> call (depends on the <code>request</code>) <code>data</code> Auxiliary data or pointer used by the syscall (e.g., value to write, pointer to structure) <code>request_name</code> Human-readable name for the <code>request</code> (e.g., <code>PTRACE_ATTACH</code>, <code>PTRACE_SYSCALL</code>)"},{"location":"database-schema/#mmap_events-table","title":"\ud83e\udde0 <code>mmap_events</code> Table","text":"Click to show table schema <pre><code>CREATE TABLE IF NOT EXISTS audit.mmap_events (\n  pid UInt32,\n  uid UInt32,\n  gid UInt32,\n  ppid UInt32,\n  user_pid UInt32,\n  user_ppid UInt32,\n  cgroup_id UInt64,\n  cgroup_name String,\n  comm String,\n\n  addr UInt64,\n  len UInt64,\n  prot UInt64,\n  flags UInt64,\n  fd UInt64,\n  off UInt64,\n\n  monotonic_ts_enter_ns UInt64,\n  monotonic_ts_exit_ns UInt64,\n  return_code Int64,\n  latency_ns UInt64,\n\n  event_type String,\n  node_name String,\n  user String,\n\n  latency_ms Float64, \n  wall_time_ms Int64,\n  wall_time_dt DateTime64(3),\n\n  container_id String,\n  container_image String,\n  container_labels_json JSON\n)\nENGINE = MergeTree()\nORDER BY wall_time_ms;\n</code></pre>"},{"location":"database-schema/#field-descriptions_3","title":"\ud83d\udd0d Field Descriptions","text":"<p>Includes all common fields described above, plus:</p> Field Description <code>addr</code> Starting address of the mapped memory region <code>len</code> Length of the memory mapping in bytes <code>prot</code> Protection flags (e.g., <code>PROT_READ</code>, <code>PROT_WRITE</code>, <code>PROT_EXEC</code>) <code>flags</code> Mapping flags (e.g., <code>MAP_PRIVATE</code>, <code>MAP_ANONYMOUS</code>, <code>MAP_SHARED</code>) <code>fd</code> File descriptor, or <code>-1</code> if the mapping is anonymous <code>off</code> Offset into the file from which mapping starts"},{"location":"database-schema/#mount_events-table","title":"\ud83d\uddc2\ufe0f <code>mount_events</code> Table","text":"Click to show table schema <pre><code>CREATE TABLE IF NOT EXISTS audit.mount_events (\n  pid UInt32,\n  uid UInt32,\n  gid UInt32,\n  ppid UInt32,\n  user_pid UInt32,\n  user_ppid UInt32,\n  cgroup_id UInt64,\n  cgroup_name String,\n  comm String,\n\n  dev_name String,\n  dir_name String,\n  type String,\n  flags UInt64,\n\n  monotonic_ts_enter_ns UInt64,\n  monotonic_ts_exit_ns UInt64,\n  return_code Int64,\n  latency_ns UInt64,\n\n  event_type String,\n  node_name String,\n  user String,\n\n  latency_ms Float64, \n  wall_time_ms Int64,\n  wall_time_dt DateTime64(3),\n\n  container_id String,\n  container_image String,\n  container_labels_json JSON\n)\nENGINE = MergeTree()\nORDER BY wall_time_ms;\n</code></pre>"},{"location":"database-schema/#field-descriptions_4","title":"\ud83d\udd0d Field Descriptions","text":"<p>Includes all common fields described above, plus:</p> Field Description <code>dev_name</code> Source device or pseudo-device (e.g., <code>proc</code>, <code>overlay</code>, <code>tmpfs</code>) <code>dir_name</code> Target directory where the filesystem is to be mounted <code>type</code> Filesystem type (e.g., <code>ext4</code>, <code>overlay</code>, <code>nfs</code>) <code>flags</code> Mount flags (e.g., <code>MS_RDONLY</code>, <code>MS_NOSUID</code>, <code>MS_BIND</code>) as raw bitmask"},{"location":"database-schema/#resource_events-table","title":"\ud83d\uddc2\ufe0f <code>resource_events</code> Table","text":"Click to show table schema <pre><code>CREATE TABLE IF NOT EXISTS audit.resource_events (\n  pid UInt32,\n  comm String,\n\n  uid UInt32,\n  gid UInt32,\n  ppid UInt32,\n  user_pid UInt32,\n  user_ppid UInt32,\n  cgroup_id UInt64,\n  cgroup_name String,\n  user String,\n\n  cpu_ns UInt64,\n  user_faults UInt64,\n  kernel_faults UInt64,\n  vm_mmap_bytes UInt64,\n  vm_munmap_bytes UInt64,\n  vm_brk_grow_bytes UInt64,\n  vm_brk_shrink_bytes UInt64,\n  bytes_written UInt64,\n  bytes_read UInt64,\n  isActive UInt32,\n\n  wall_time_dt DateTime64(3),\n  wall_time_ms Int64,\n\n  container_id String,\n  container_image String,\n  container_labels_json JSON\n\n) ENGINE = MergeTree()\nORDER BY wall_time_ms;\n</code></pre>"},{"location":"database-schema/#field-descriptions_5","title":"\ud83d\udd0d Field Descriptions","text":"<p>Includes all common fields described above, plus:</p> Field Description <code>cpu_ns</code> Total CPU time consumed by the task in nanoseconds (from context switch) <code>user_faults</code> Number of page faults occurring in user space <code>kernel_faults</code> Number of page faults occurring in kernel space <code>vm_mmap_bytes</code> Total bytes allocated via <code>mmap</code> <code>vm_munmap_bytes</code> Total bytes released via <code>munmap</code> <code>vm_brk_grow_bytes</code> Bytes of heap memory grown via <code>brk</code> <code>vm_brk_shrink_bytes</code> Bytes of heap memory released via <code>brk</code> <code>bytes_written</code> Total bytes written by the process (from syscalls like <code>write</code>) <code>bytes_read</code> Total bytes read by the process (from syscalls like <code>read</code>) <code>isActive</code> Indicates if the process is still active (<code>1</code>) or has exited (<code>0</code>)"},{"location":"database-schema/#syscall_freq_events-table","title":"\ud83d\uddc2\ufe0f <code>syscall_freq_events</code> Table","text":"Click to show table schema <pre><code>CREATE TABLE IF NOT EXISTS audit.syscall_freq_events (\n  pid UInt32,\n  comm String,\n  uid UInt32,\n  gid UInt32,\n  ppid UInt32,\n  user_pid UInt32,\n  user_ppid UInt32,\n  cgroup_id UInt64,\n  cgroup_name String,\n  user String,\n  syscall_vector_json JSON, \n  wall_time_dt DateTime64(3),\n  wall_time_ms Int64,\n  container_id String,\n  container_image String,\n  container_labels_json JSON\n)\nENGINE = MergeTree()\nORDER BY wall_time_ms;\n</code></pre>"},{"location":"database-schema/#field-descriptions_6","title":"\ud83d\udd0d Field Descriptions","text":"<p>Includes all common fields described above, plus:</p> Field Description <code>syscall_vector_json</code> JSON object mapping syscall numbers to their invocation counts.  Example: <code>{\"0\":12,\"1\":4,\"60\":9}</code> meaning <code>read</code> was called 12 times, <code>write</code> 4 times, <code>exit</code> 9 times."},{"location":"demo/","title":"Demo","text":""},{"location":"demo/#demo-video","title":"\ud83d\udcfa Demo Video","text":"<p>Watch the InfraSight demo in action:</p>"},{"location":"ebpf-programs/","title":"eBPF Programs","text":""},{"location":"ebpf-programs/#ebpf-programs-and-attach-points","title":"\u2699\ufe0f eBPF Programs and Attach Points","text":"<p>InfraSight uses a suite of eBPF programs to trace specific kernel-level events. Each program is attached to carefully chosen tracepoints or kprobes to monitor relevant syscall or kernel function activity.</p>"},{"location":"ebpf-programs/#overview","title":"\ud83d\udd0d Overview","text":"Program Attach Points Attach Type Description execve <code>sys_enter_execve</code>, <code>sys_exit_execve</code> Tracepoint Traces process execution events, including command-line arguments and exit status open <code>sys_enter_openat</code>, <code>sys_exit_openat</code> Tracepoint Captures file open attempts, including accessed filename chmod <code>sys_enter_fchmodat</code>, <code>sys_exit_fchmodat</code> Tracepoint Monitors changes to file permissions accept <code>inet_csk_accept</code> (entry and return) kprobe / kretprobe Captures accepted network connections (i.e., incoming TCP connections) connect <code>tcp_v4_connect</code>, <code>tcp_v6_connect</code> (entry and return) kprobe / kretprobe Monitors outbound TCP connection attempts for both IPv4 and IPv6 ptrace <code>sys_enter_ptrace</code>, <code>sys_exit_ptrace</code> Tracepoint Observes process tracing actions like <code>attach</code>, <code>peek</code>, <code>poke</code>, and <code>continue</code>; useful for detecting debuggers, tampering, or reverse engineering mmap <code>sys_enter_mmap</code>, <code>sys_exit_mmap</code> Tracepoint Tracks memory mapping requests, including suspicious RWX regions used in shellcode or injection attacks mount <code>sys_enter_mount</code>, <code>sys_exit_mount</code> Tracepoint Monitors mount system calls useful for detecting container mount propagation, overlay mounts, or filesystem tampering umount <code>sys_enter_umount</code>, <code>sys_exit_umount</code> Tracepoint Tracks unmount operations, which may indicate container teardown, cleanup, or attempts to hide malicious filesystems Resource Tracer - <code>kprobe/finish_task_switch</code>  - <code>tracepoint/exceptions/page_fault_user</code>  - <code>tracepoint/exceptions/page_fault_kernel</code>  - <code>tracepoint/syscall/sys_enter_mmap</code>  - <code>tracepoint/syscalls/sys_enter_munmap</code>  - <code>tracepoint/syscalls/sys_exit_munmap</code>  - <code>tracepoint/syscalls/sys_exit_brk</code>  - <code>tracepoint/syscalls/sys_exit_read</code>  - <code>tracepoint/syscalls/sys_exit_write</code>  - <code>tracepoint/sched/sched_process_exit</code> Kprobe + Tracepoints Monitors low-level resource usage and memory management (context switches, page faults, mmap/munmap, brk, read/write, and process exit). Useful for detecting anomalous resource consumption or crashes. Syscall Freq Tracer <code>tracepoint/raw_syscalls/sys_enter</code>, <code>tracepoint/sched/sched_process_exit</code> Tracepoint Counts syscall invocations and aggregates frequency metrics per process until exit. Useful for anomaly detection based on unusual syscall usage patterns."},{"location":"ebpf-programs/#attach-types-explained","title":"\ud83e\udde9 Attach Types Explained","text":"<ul> <li>Tracepoint: Static instrumentation points in the kernel. Safer and more stable across kernel versions. Used for syscalls like <code>execve</code>, <code>open</code>, <code>chmod</code>.</li> <li>kprobe / kretprobe: Dynamic probes on kernel functions. Used for networking-related functions like <code>inet_csk_accept</code> and <code>tcp_v*_connect</code>.</li> </ul>"},{"location":"ebpf-programs/#program-location","title":"\ud83d\udcc1 Program Location","text":"<p>Each eBPF program is written in C and located under the <code>bpf/</code> directory of the <code>ebpf_loader</code> repository. Here are the links to each specific tracer:</p> <ul> <li>execve_tracer \u2013 Monitors process execution</li> <li>open_tracer \u2013 Tracks file open activity</li> <li>chmod_tracer \u2013 Observes permission changes</li> <li>accept_tracer \u2013 Hooks accepted TCP connections</li> <li>connect_tracer \u2013 Tracks outbound TCP connection attempts</li> <li>ptrace_tracer \u2013 Detects process tracing and debugging behavior such as <code>ptrace</code> attach, memory read/write, and syscall control</li> <li>mmap_tracer \u2013 Observes memory allocation via <code>mmap</code> useful for detecting RWX mappings, shellcode injection, or memory-based exploits</li> <li>mount_tracer \u2013 Monitors filesystem mount operations useful for observing overlay mounts or suspicious remounts</li> <li>umount_tracer \u2013 Monitors filesystem unmount operations, useful for detecting container shutdowns, cleanup routines, or attempts to hide activity by unmounting evidence</li> <li>resource_tracer \u2013 Provides visibility into low-level resource and memory usage. It hooks context switches, page faults, memory mappings/unmappings, <code>brk</code>, read/write syscalls, and process exits. This tracer is useful for detecting anomalous resource consumption, memory pressure, or potential exploitation attempts.</li> <li>syscall_freq \u2013 Tracks the frequency of syscalls per process by hooking into raw syscall entries and process exits. The collected counts are useful for anomaly detection and behavioral baselining.</li> </ul> <p>These programs are compiled using <code>bpf2go</code>, a tool from the Cilium/ebpf project, which generates Go bindings for eBPF C code. The resulting artifacts are then loaded dynamically by the <code>ebpf_loader</code> agent at runtime.</p>"},{"location":"getting-started/","title":"Getting Started","text":""},{"location":"getting-started/#getting-started","title":"\ud83d\ude80 Getting Started","text":"<p>InfraSight is designed as a modular and flexible observability platform. Each component operates independently and can be deployed on its own or as part of the full stack. You can run InfraSight in two main environments:</p>"},{"location":"getting-started/#standalone-linux-environments","title":"Standalone Linux Environments","text":"<p>You can deploy InfraSight components directly on a Linux host using:</p> <ul> <li>A prebuilt executable binary (ideal for simple or resource-limited setups)</li> <li>A Docker container, which encapsulates dependencies and makes deployment easier across systems</li> </ul> <p>This mode is well-suited for single-node monitoring, embedded systems, or edge devices.</p>"},{"location":"getting-started/#kubernetes-clusters","title":"Kubernetes Clusters","text":"<p>InfraSight can also be deployed in a Kubernetes cluster using Helm charts and custom controllers. This mode enables scalable, multi-node observability and is ideal for modern infrastructure.</p>"},{"location":"getting-started/#component-overview","title":"\ud83d\udce6 Component Overview","text":"<p>Each core module of InfraSight lives in its own GitHub repository and includes setup and configuration details in its respective <code>README.md</code>. Follow the links below to get started with each part:</p> Component Description <code>ebpf_loader</code> Lightweight agent that runs eBPF programs and sends enriched telemetry via gRPC <code>ebpf_server</code> Central gRPC server that enriches and stores eBPF data into ClickHouse <code>infrasight-controller</code> Kubernetes operator that automates the lifecycle of eBPF agents <code>ebpf_deploy</code> Helm charts to deploy the complete stack, including ClickHouse and InfraSight components <code>InfraSight_ml</code> Machine learning models for anomaly detection, consuming telemetry from Kafka <code>InfraSight_sentinel</code> Rules engine that applies predefined detection logic to events from Kafka and generates alerts <p>Each repository provides:</p> <ul> <li>Clear installation instructions</li> <li>Example configuration files</li> <li>Guidance on running in different environments</li> </ul> <p>Whether you want to monitor a single host or your entire cluster, you can start with just one module or deploy the entire InfraSight stack.</p> <p>Tip: Start by reviewing the README in each repository to see how the pieces fit together in your environment.</p>"}]}